\documentclass[main.tex]{subfiles}

\begin{document}

\chapter{Testing}

\section{Step Dectection}

Step detection is an integral part of any dead-reckoning system and such the method of identifying these steps should be tested and evaulated to find how much error the system has a whole. As mentioned previously, different methods were implemented to try and find the best step detection system, resulting in three step detection algorithm implmentations that needed testing:
\begin{enumerate}
	\item First Order Low Pass Filter
	\item 10th Order Butterworth Filter
	\item 5th Order Butterworth Filter
\end{enumerate}
The Basic Peak-Valley detection was considered for testing as it forms the basis of how steps are detected in all future methods, making it a useful baseline comparison for all results. However, if it is completely unfiltered, then it will simply accept most actions as a step, whilst if it is flitered, then often no action will trigger a step. As such, it is not suited for analysis as a standalone entity, forcing us to forgo these plans.

All tests are done on a device of each operating system (Android/iOS) with different users. This should allow for these tests to prove that the application works independently of user and device.

\subsection{Error Rate}

The error rate for the Step Detection algorithms is defined as follows:

\begin{equation}
Error\ Rate = 1 - \frac{Counted\ Value}{True\ Value}
\end{equation}

Over a given walk, the number of steps counted by the algorithm is divided by the true value of steps taken to determine the proportion of steps detected. Subtracting this value from 1 gives an overall error rate, with values close to 0 indicating favourable error rates.

%So over a known number of steps, True Value, each algorithm is used and its counted number of steps is used to get it's accuracy. From this is then subtracted from 1 to get the error rate. The purpose is to see over a set distance how much error is present overall regardless of timing or other factors such as speed, location etc... This should help to see how much where in the future the system can be improved.


\begin{table}[ht]
	\begin{tabular}{| c || c | c | c | c | c | c | c | c | c | c | c | c |}
		\hline
		True & \multicolumn{4}{|c|}{Low Pass} & \multicolumn{4}{|c|}{5th Butterworth} & \multicolumn{4}{|c|}{10th Butterworth} \\
		\hline
		 - & Andr. & Err. & iOS & Err. & Andr. & Err. & iOS & Err. & Andr. & Err. & iOS & Err.\\
		\hline
		10 & 9 & 0.1 & x & x & 10 & 0.0 & x & x & 10 & 0.0 & x & x\\ 		
		25 & 22 & 0.12 & x & x & 25 & 0.0 & x & x & 26 & -0.04 & x & x \\ 	
		50 & 49 & 0.02 & x & x & 50 & 0.0 & x & x & 51 & -0.02 & x & x\\ 
		\hline	
	\end{tabular}
	\caption{The raw recorded values for each method, Andr. = Androird, Err. = Error Rate}
\end{table}

The above table shows the number of steps the each step detector counter against the true value. Three different true values where selected to see whether distance would have an impact on error build up. The highest true value, 50, was selected because this is roughly at most the number of steps a user will take within a single floor in our environment. Then when transitioning to another floor the error should be reset. From the table above it can be see that the error does not build up for within the step detector algorithms, as the error rate stays fairly constant between the distances. 

This lack of error build up is to be expected as they step detectors are identifying a specific pattern within a filtered reading from the accelerometer, which does not tend to drift.

However the main point of interest from this data is that the 5th Order Butterworth method has a perfect detection rate and therefore would be better than the small amount of error from the 10th Order one. Despite this the choice to move from 10th to 5th Order was due to the lag that the 10th Order filter caused on the smartphones. The accuracy difference is minimal as well to have next to no effect. 

In this particular case, we considered that the underestimation and overestimation of steps are both equally detrimental, since both would result in the addition of an equal amount of error to the system.

%Though overcounting and getting a negative error rate could be considered worse than undercounting. This is because if you overcount it means that for a single step you will have counted an additional step, and in a dead-reckoning system the user will now have moved double the distance in a given direction by your estimate. This could have a larger effect on the error in the system.

\subsection{Proximity to Destination}

The previous test simply looks at the step detector methods in an isolated environment, to give a picture of their performance with the whole application, they were then tested against each other in the full application. Each algorithm was used in the application along 2 routes throughout the department:

\begin{enumerate}
	\item Room-to-Room, so from the entrance of one room to another  %, on the same floor
	\item Point to Point, from a specific starting location to a specific destination  %, on the same floor
%	\item Room to Room, across multiple floors
%	\item Point to Point across multiple floors
\end{enumerate}

Upon the application stating that the destination had been reached, the distance between the users location and the actual location of the destination was measured in metres.

The purpose of this final test was to examine how well each implementation would handle working in the full environment with other factors. It also allows for a more tangible look into how accurate the system is, as opposed to just producing statistical figures. However, this does introduce some additional error from the heading calculations and other aspects of the application.

%Results

\subsection{Other Considered Tests}

Two other tests where considered for use, but ultimately decided against for different reasons. The first was trying to measure if each step detector method was overcounting, registering multiple steps instead when it should only register a single. This is different from what was defined as a false positive earlier where a step is counted when no movement is occuring. This was considered as an alternative to the false positive rate as it looks at the way in which a bad algorithm could have a low error rate but not count correctly. 

However whilst this method could provide some analysis, it was not used for two reasons:

\begin{enumerate}
	\item All the methods being tested do not overcount often, it only really differentiates them from the basic unfiltered version.
	\item The second test that was not used would be better at showing which algorithms are working correctly
\end{enumerate}

The second test that was excluded was aiming to see if step detectors worked effectively over a period of time. The experiment was based off the fact that a person moving at a constant speed will have a repetitive waveform as the input into the algorithms. Therefore the steps being detected should be picked up constantly and equally spaced in time. The purpose of this test is to therefore show that an algorithm is matching the constant pattern and working as intended, it will show up any method that has a low error rate but doesn't work as intended. This was considered a better way to show what the overcounting test would have provided.

Whilst the method had some potential, early data recordings showed that all of the algorithms being tested had roughly the same pattern. They had the same pattern due to the need to have the timing relative to when it itself recorded the first step. Even though some methods recorded steps comparatively longer after they were actually taken, the timing results did not reflect that since the relative difference between when the steps as recorded by the algorithms themselves was similar


\end{document}